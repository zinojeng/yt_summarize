import streamlit as st
from dotenv import load_dotenv
import os
from elevenlabs_stt import transcribe_audio_elevenlabs
from whisper_stt import (
    transcribe_audio_whisper,
    get_model_description
)
from transcript_refiner import refine_transcript
from utils import check_file_size, split_large_audio
import logging
import tempfile
from openai import OpenAI
import google.generativeai as genai
from pydub import AudioSegment

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å®šç¾©å¯ç”¨çš„ OpenAI æ¨¡å‹
AVAILABLE_MODELS = {
    "gpt-4o": "gpt-4o",
    "gpt-4o-mini": "gpt-4o-mini",
    "o3-mini": "o3-mini",
    "o1-mini": "o1-mini"
}

# æ¨¡å‹è¨­å®šå’Œåƒ¹æ ¼ï¼ˆUSD per 1M tokensï¼‰
MODEL_CONFIG = {
    "gpt-4o": {
        "display_name": "gpt-4o",
        "input": 2.50,          # $2.50 per 1M tokens
        "cached_input": 1.25,   # $1.25 per 1M tokens
        "output": 10.00         # $10.00 per 1M tokens
    },
    "gpt-4o-mini": {
        "display_name": "gpt-4o-mini",
        "input": 0.15,          # $0.15 per 1M tokens
        "cached_input": 0.075,  # $0.075 per 1M tokens
        "output": 0.60          # $0.60 per 1M tokens
    },
    "o1-mini": {
        "display_name": "o1-mini",
        "input": 1.10,          # $1.10 per 1M tokens
        "cached_input": 0.55,   # $0.55 per 1M tokens
        "output": 4.40          # $4.40 per 1M tokens
    },
    "o3-mini": {
        "display_name": "o3-mini",
        "input": 1.10,          # $1.10 per 1M tokens
        "cached_input": 0.55,   # $0.55 per 1M tokens
        "output": 4.40          # $4.40 per 1M tokens
    },
    "gemini-2.5-flash-preview-05-20": {
        "display_name": "Gemini 2.5 Pro Experimental",
        "input": 0.00,          # åƒ¹æ ¼å¾…å®š
        "cached_input": 0.00,   # åƒ¹æ ¼å¾…å®š
        "output": 0.00          # åƒ¹æ ¼å¾…å®š
    }
}

# åŒ¯ç‡è¨­å®š
USD_TO_NTD = 31.5

# è½‰éŒ„æœå‹™èªªæ˜
TRANSCRIPTION_SERVICE_INFO = {
    "Whisper": """
    ### Whisper æ¨¡å‹
    - é–‹æºçš„èªéŸ³è½‰æ–‡å­—æ¨¡å‹
    - æ”¯æ´å¤šç¨®èªè¨€
    - å¯é›¢ç·šä½¿ç”¨
    """,
    "ElevenLabs": """
    ### ElevenLabs æ¨¡å‹
    - å•†æ¥­ç´šèªéŸ³è½‰æ–‡å­—æœå‹™
    - æ”¯æ´ 99 ç¨®èªè¨€
    - æä¾›èªªè©±è€…è¾¨è­˜åŠŸèƒ½
    """,
    "OpenAI 2025 New": """
    ### OpenAI 2025 å…¨æ–°æ¨¡å‹
    - gpt-4o-transcribeï¼šé«˜ç²¾åº¦ã€å¤šèªè¨€æ”¯æ´
    - gpt-4o-mini-transcribeï¼šè¼•é‡å¿«é€Ÿã€æ€§åƒ¹æ¯”é«˜
    - è‡ªå‹•èªè¨€æª¢æ¸¬
    - æ›´å¥½çš„ä¸­æ–‡è½‰éŒ„æ•ˆæœ
    """
}

# å„ªåŒ–æœå‹™èªªæ˜
OPTIMIZATION_SERVICE_INFO = {
    "OpenAI": """
    ### OpenAI å„ªåŒ–æ¨¡å‹
    - å°ˆæ¥­çš„æ–‡å­—å„ªåŒ–å’Œæ ¡æ­£
    - æ”¯æ´å¤šç¨®èªè¨€
    - å¯è‡ªè¨‚å„ªåŒ–ç¨‹åº¦
    """,
    "Gemini": """
    ### Google Gemini 2.5 Pro (å¯¦é©—æ€§)
    - æœ€æ–°çš„ Google AI æ¨¡å‹
    - æ›´å¼·çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
    - æ›´è‡ªç„¶çš„èªè¨€è™•ç†
    - æ”¯æ´å¤šèªè¨€å„ªåŒ–
    - å¯¦é©—æ€§åŠŸèƒ½ï¼ŒæŒçºŒæ”¹é€²ä¸­
    """
}

def refine_transcript_gemini(text, api_key, temperature=0.5, context=""):
    """ä½¿ç”¨ Gemini æ¨¡å‹å„ªåŒ–æ–‡å­—

    Args:
        text (str): è¦å„ªåŒ–çš„æ–‡å­—
        api_key (str): Gemini API é‡‘é‘°
        temperature (float): å‰µæ„ç¨‹åº¦ (0.0-1.0)
        context (str): ä¸Šä¸‹æ–‡æç¤º

    Returns:
        dict: åŒ…å«å„ªåŒ–å¾Œçš„æ–‡å­—å’Œæ‘˜è¦
    """
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')
        
        # æº–å‚™æç¤ºè©
        prompt = f"""
        # æŒ‡ä»¤ï¼šè£½ä½œè©³ç´°å…§å®¹ç­†è¨˜èˆ‡æ·±åº¦åˆ†æ (å°ˆæ¥­æ•´ç†ç‰ˆ)

        è«‹å°‡ä»¥ä¸‹æä¾›çš„æ–‡æœ¬å„ªåŒ–ç‚ºä¸€ä»½ **æ·±åº¦åˆ†æã€çŸ¥è­˜è±å¯Œã€çµæ§‹æ¸…æ™°** çš„å°ˆæ¥­ç­†è¨˜ã€‚
        **ç„¡è«–è¼¸å…¥æ–‡å­—æ˜¯ç°¡é«”æˆ–ç¹é«”ä¸­æ–‡ï¼Œè«‹å‹™å¿…å°‡æ‰€æœ‰è¼¸å‡ºè½‰æ›ç‚ºã€ç¹é«”ä¸­æ–‡ã€‘ã€‚**

        ## ä»»å‹™è¦æ±‚

        1.  **æ·±åº¦åˆ†æè¦æ±‚**
            *   æä¾›å°æ ¸å¿ƒæ¦‚å¿µçš„**æ·±å…¥è§£é‡‹**ï¼Œä¸åƒ…æ‘˜è¦å…§å®¹ï¼Œé‚„è¦æ¢è¨å…¶èƒŒå¾Œçš„åŸç†èˆ‡æ„ç¾©ã€‚
            *   è­˜åˆ¥å…§å®¹ä¸­çš„**æŠ€è¡“ç´°ç¯€**ã€**å¯¦å‹™æ‡‰ç”¨**å’Œ**å°ˆæ¥­æ´å¯Ÿ**ã€‚
            *   åˆ†æå…§å®¹ä¸­**å¯èƒ½çš„å½±éŸ¿**å’Œ**æœªä¾†ç™¼å±•è¶¨å‹¢**ã€‚
            *   ä¿æŒ**å°ˆæ¥­æº–ç¢º**çš„è©å½™å’Œè¡¨é”ã€‚

        2.  **çµæ§‹åŒ–è¼¸å‡ºè¦æ±‚**
            *   è£½ä½œä¸€ä»½å…¨é¢çš„**å…§å®¹å¤§ç¶±**ï¼ˆåŒ…å« 5-8 å€‹ä¸»è¦éƒ¨åˆ†ï¼‰ã€‚
            *   æ¯å€‹éƒ¨åˆ†éœ€è¦æœ‰**å°æ¨™é¡Œ**å’Œ**è©³ç´°å…§å®¹**ã€‚
            *   é‡é»æ¨™è¨˜**é—œéµæ¦‚å¿µ**å’Œ**æŠ€è¡“è¡“èª**ã€‚
            *   åŒ…å«**é‡è¦å¼•è¿°**æˆ–**æ¡ˆä¾‹ç ”ç©¶**çš„è©³ç´°èªªæ˜ã€‚
            *   åŠ å…¥**å¯¦è¸å»ºè­°**å’Œ**æ‡‰ç”¨å ´æ™¯**çš„åˆ†æã€‚

        3.  **æ ¼å¼èˆ‡æ’ç‰ˆè¦æ±‚** (è«‹åš´æ ¼éµå®ˆ)
            *   **æ¨™é¡Œå±¤ç´š**: ä½¿ç”¨ `#` `##` `###` å€åˆ†ä¸»é¡Œå€å¡Š (ä¾‹å¦‚ï¼š`## **å…§å®¹å¤§ç¶±**`)ã€‚
            *   **åˆ†éš”ç·š**: *åƒ…åœ¨* ä¸»è¦å€å¡Šä¹‹é–“ä½¿ç”¨ `---` åˆ†éš”ç·šã€‚
            *   **ç²—é«”**: 
                *   åƒ…ä½¿ç”¨ `**ç²—é«”**` æ¨™ç¤º **å€å¡Šæ¨™é¡Œæœ¬èº«** (ä¾‹å¦‚ï¼š`## **å…§å®¹å¤§ç¶±**`)ã€‚
                *   æ–‡æœ¬ä¸­çš„**é—œéµè©**å’Œ**é‡è¦æ¦‚å¿µ**å¯ä»¥è¨­ç‚ºç²—é«”ã€‚
            *   **åˆ—è¡¨**: ä½¿ç”¨ `-` æˆ– `*` è£½ä½œé …ç›®æ¸…å–®ï¼Œç”¨æ–¼åˆ—èˆ‰è¦é»ã€‚
            *   **å¼•ç”¨**: ä½¿ç”¨ `>` æ¨™è¨˜åŸå§‹å…§å®¹ä¸­çš„é‡è¦èªå¥ã€‚
            *   **ä»£ç¢¼å¡Š**: ä½¿ç”¨ ``` åŒ…è£¹æŠ€è¡“ç´°ç¯€æˆ–ç‰¹å®šç¨‹å¼ç¢¼ï¼ˆå¦‚é©ç”¨ï¼‰ã€‚

        # ä¸Šä¸‹æ–‡è³‡è¨Š
        {context if context else "ç„¡ç‰¹å®šä¸Šä¸‹æ–‡"}

        # åŸå§‹æ–‡å­—
        {text}

        # è¼¸å‡ºçµæ§‹è¦æ±‚ (å°ˆæ¥­æ·±åº¦åˆ†æç‰ˆ)

        è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹çµæ§‹å’Œ Markdown æ ¼å¼ç”Ÿæˆå…§å®¹ï¼Œæ‰€æœ‰å…§å®¹å‡ç‚º**ç¹é«”ä¸­æ–‡**ï¼Œä¸¦ç¢ºä¿**å…§å®¹è±å¯Œä¸”æ·±å…¥**ï¼š

        ## **ä¸»è¦è§€é»èˆ‡æ ¸å¿ƒåƒ¹å€¼**
        (æä¾› 600-800 å­—çš„æ·±åº¦åˆ†æï¼Œé—¡è¿°å…§å®¹çš„æ ¸å¿ƒè§€é»å’Œåƒ¹å€¼)

        ---
        ## **å…§å®¹å¤§ç¶±**
        1. (ç¬¬ä¸€éƒ¨åˆ†æ¨™é¡Œ)
        2. (ç¬¬äºŒéƒ¨åˆ†æ¨™é¡Œ)
        3. (ç¬¬ä¸‰éƒ¨åˆ†æ¨™é¡Œ)
        4. (ç¬¬å››éƒ¨åˆ†æ¨™é¡Œ)
        5. (ç¬¬äº”éƒ¨åˆ†æ¨™é¡Œ)
        (è¦–å…§å®¹è¤‡é›œåº¦å¯å¢åŠ è‡³6-8å€‹éƒ¨åˆ†)

        ---
        ## **é—œéµè¡“èªèˆ‡æ¦‚å¿µ**
        - (è¡“èª1): (æ¸…æ™°æº–ç¢ºçš„å®šç¾©èˆ‡èªªæ˜)
        - (è¡“èª2): (æ¸…æ™°æº–ç¢ºçš„å®šç¾©èˆ‡èªªæ˜)
        - (è¡“èª3): (æ¸…æ™°æº–ç¢ºçš„å®šç¾©èˆ‡èªªæ˜)
        - (è¡“èª4): (æ¸…æ™°æº–ç¢ºçš„å®šç¾©èˆ‡èªªæ˜)
        - (è¡“èª5): (æ¸…æ™°æº–ç¢ºçš„å®šç¾©èˆ‡èªªæ˜)

        ---
        ## **é‡è¦å¼•è¿°èˆ‡æ¡ˆä¾‹**
        > (é‡è¦å¼•è¿°1)
        åˆ†æ: (å°æ­¤å¼•è¿°çš„æ·±åº¦è§£æï¼ŒåŒ…å«èƒŒæ™¯å’Œæ„ç¾©)

        > (é‡è¦å¼•è¿°2)
        åˆ†æ: (å°æ­¤å¼•è¿°çš„æ·±åº¦è§£æï¼ŒåŒ…å«èƒŒæ™¯å’Œæ„ç¾©)

        ---
        ## **è©³ç´°å…§å®¹åˆ†æ**
        ### (ç¬¬ä¸€éƒ¨åˆ†æ¨™é¡Œ)
        (æ­¤è™•æä¾›300-500å­—çš„æ·±å…¥åˆ†æï¼ŒåŒ…å«æ ¸å¿ƒæ¦‚å¿µè§£é‡‹ã€æŠ€è¡“ç´°ç¯€ã€ç¯„ä¾‹èªªæ˜ç­‰)

        ### (ç¬¬äºŒéƒ¨åˆ†æ¨™é¡Œ)
        (æ­¤è™•æä¾›300-500å­—çš„æ·±å…¥åˆ†æï¼ŒåŒ…å«æ ¸å¿ƒæ¦‚å¿µè§£é‡‹ã€æŠ€è¡“ç´°ç¯€ã€ç¯„ä¾‹èªªæ˜ç­‰)

        (ä»¥æ­¤é¡æ¨å®Œæˆæ‰€æœ‰éƒ¨åˆ†çš„è©³ç´°åˆ†æ)

        ---
        ## **å¯¦è¸æ‡‰ç”¨èˆ‡å»ºè­°**
        - (å»ºè­°1): (é‡å°æ­¤å»ºè­°çš„è©³ç´°èªªæ˜å’Œå¯¦æ–½æ–¹æ³•)
        - (å»ºè­°2): (é‡å°æ­¤å»ºè­°çš„è©³ç´°èªªæ˜å’Œå¯¦æ–½æ–¹æ³•)
        - (å»ºè­°3): (é‡å°æ­¤å»ºè­°çš„è©³ç´°èªªæ˜å’Œå¯¦æ–½æ–¹æ³•)

        ---
        ## **ç›¸é—œè³‡æºèˆ‡å»¶ä¼¸é–±è®€**
        - (è³‡æº1): (è³‡æºèªªæ˜å’Œåƒ¹å€¼)
        - (è³‡æº2): (è³‡æºèªªæ˜å’Œåƒ¹å€¼)
        - (è³‡æº3): (è³‡æºèªªæ˜å’Œåƒ¹å€¼)
        """
        
        response = model.generate_content(
            prompt,
            generation_config={
                'temperature': temperature
            }
        )
        
        # è§£æå›æ‡‰
        response_text = response.text
        
        # ä½¿ç”¨æ–°çš„åˆ†éš”æ–¹å¼è§£æå›æ‡‰
        if "## **ä¸»è¦è§€é»èˆ‡æ ¸å¿ƒåƒ¹å€¼**" in response_text and "## **è©³ç´°å…§å®¹åˆ†æ**" in response_text:
            parts = response_text.split("## **è©³ç´°å…§å®¹åˆ†æ**")
            summary = parts[0].split("## **ä¸»è¦è§€é»èˆ‡æ ¸å¿ƒåƒ¹å€¼**")[1].strip()
            corrected = "## **è©³ç´°å…§å®¹åˆ†æ**" + parts[1].strip()
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ¨™è¨˜ï¼Œè¿”å›å®Œæ•´å›æ‡‰
            corrected = response_text
            summary = "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
        
        return {
            "corrected": corrected,
            "summary": summary,
            "usage": {
                "total_input_tokens": 0,  # Gemini æš«æ™‚ä¸è¨ˆç®— tokens
                "total_output_tokens": 0
            }
        }
    except Exception as e:
        logger.error(f"Gemini API éŒ¯èª¤ï¼š{str(e)}")
        return None

def calculate_cost(input_tokens, output_tokens, model_name, is_cached=False):
    """è¨ˆç®— API ä½¿ç”¨æˆæœ¬
    
    Args:
        input_tokens (int): è¼¸å…¥ tokens æ•¸é‡
        output_tokens (int): è¼¸å‡º tokens æ•¸é‡
        model_name (str): æ¨¡å‹åç¨±
        is_cached (bool, optional): æ˜¯å¦ä½¿ç”¨å¿«å–è¼¸å…¥åƒ¹æ ¼. é è¨­ç‚º False
    
    Returns:
        tuple: (USD æˆæœ¬, NTD æˆæœ¬, è©³ç´°è¨ˆç®—è³‡è¨Š)
    """
    if model_name not in MODEL_CONFIG:
        return 0, 0, "æœªæ”¯æ´çš„æ¨¡å‹"
        
    # å–å¾—åƒ¹æ ¼è¨­å®š
    model = MODEL_CONFIG[model_name]
    input_price = model["cached_input"] if is_cached else model["input"]
    output_price = model["output"]
    
    # è¨ˆç®— USD æˆæœ¬ (ä»¥æ¯ 1M tokens ç‚ºå–®ä½)
    input_cost = (input_tokens / 1_000_000) * input_price
    output_cost = (output_tokens / 1_000_000) * output_price
    total_cost_usd = input_cost + output_cost
    total_cost_ntd = total_cost_usd * USD_TO_NTD
    
    # æº–å‚™è©³ç´°è¨ˆç®—è³‡è¨Š
    details = f"""
    è¨ˆç®—æ˜ç´° (USD):
    - è¼¸å…¥: {input_tokens:,} tokens Ã— ${input_price}/1M = ${input_cost:.4f}
    - è¼¸å‡º: {output_tokens:,} tokens Ã— ${output_price}/1M = ${output_cost:.4f}
    - ç¸½è¨ˆ (USD): ${total_cost_usd:.4f}
    - ç¸½è¨ˆ (NTD): NT${total_cost_ntd:.2f}
    """
    return total_cost_usd, total_cost_ntd, details


def display_cost_info(
    input_tokens,
    output_tokens,
    model_name,
    is_cached=False
):
    """åœ¨ Streamlit ä»‹é¢ä¸­é¡¯ç¤ºæˆæœ¬è³‡è¨Š"""
    cost_usd, cost_ntd, details = calculate_cost(
        input_tokens,
        output_tokens,
        model_name,
        is_cached
    )
    
    with st.sidebar.expander("ğŸ’° æˆæœ¬è¨ˆç®—", expanded=True):
        st.write("### Token ä½¿ç”¨é‡")
        st.write(f"- è¼¸å…¥: {input_tokens:,} tokens")
        st.write(f"- è¼¸å‡º: {output_tokens:,} tokens")
        st.write(f"- ç¸½è¨ˆ: {input_tokens + output_tokens:,} tokens")
        
        if (input_tokens + output_tokens) == 0:
            st.warning("ç›®å‰ token ä½¿ç”¨é‡ç‚º 0ï¼Œè«‹ç¢ºèªæ˜¯å¦å·²æ­£ç¢ºè¨ˆç®— token æ•¸é‡ï¼")
        
        st.write("### è²»ç”¨æ˜ç´°")
        st.text(details)
        
        if is_cached:
            st.info("âœ¨ ä½¿ç”¨å¿«å–åƒ¹æ ¼è¨ˆç®—")


def main():
    """ä¸»ç¨‹å¼å‡½æ•¸"""
    st.title("éŸ³è¨Šè½‰æ–‡å­—èˆ‡å„ªåŒ–ç³»çµ±")
    
    # åˆå§‹åŒ– session state
    if "transcribed_text" not in st.session_state:
        st.session_state.transcribed_text = None
    if "input_tokens" not in st.session_state:
        st.session_state.input_tokens = 0
    if "output_tokens" not in st.session_state:
        st.session_state.output_tokens = 0
    if "total_tokens" not in st.session_state:
        st.session_state.total_tokens = 0
    if "optimized_text" not in st.session_state:
        st.session_state.optimized_text = None
    if "summary_text" not in st.session_state:
        st.session_state.summary_text = None

    with st.sidebar:
        st.header("è¨­å®š")
        
        # åˆ†æˆå…©å€‹æ¨™ç±¤é ï¼šè½‰éŒ„è¨­å®šå’Œå„ªåŒ–è¨­å®š
        tab1, tab2 = st.tabs(["ğŸ™ï¸ è½‰éŒ„è¨­å®š", "âœ¨ å„ªåŒ–è¨­å®š"])
        
        # è½‰éŒ„è¨­å®šæ¨™ç±¤é 
        with tab1:
            # é¸æ“‡è½‰éŒ„æœå‹™
            transcription_service = st.selectbox(
                "é¸æ“‡è½‰éŒ„æœå‹™",
                ["OpenAI 2025 New", "Whisper", "ElevenLabs"],
                index=0,
                help="é¸æ“‡è¦ä½¿ç”¨çš„èªéŸ³è½‰æ–‡å­—æœå‹™"
            )
            
            # é¡¯ç¤ºæœå‹™èªªæ˜
            st.markdown(TRANSCRIPTION_SERVICE_INFO[transcription_service])
            
            # Whisper ç›¸é—œè¨­å®š
            if transcription_service == "Whisper":
                whisper_model = st.selectbox(
                    "é¸æ“‡ Whisper æ¨¡å‹",
                    options=["tiny", "base", "small", "medium", "large"],
                    index=2
                )
                st.session_state["whisper_model"] = whisper_model
                st.caption(get_model_description(whisper_model))
                
                # èªè¨€è¨­å®š
                language_mode = st.radio(
                    "èªè¨€è¨­å®š",
                    options=["è‡ªå‹•åµæ¸¬", "æŒ‡å®šèªè¨€", "æ··åˆèªè¨€"],
                    help="é¸æ“‡éŸ³è¨Šçš„èªè¨€è™•ç†æ¨¡å¼"
                )
                
                if language_mode == "æŒ‡å®šèªè¨€":
                    languages = {
                        "ä¸­æ–‡ (ç¹é«”/ç°¡é«”)": "zh",
                        "è‹±æ–‡": "en",
                        "æ—¥æ–‡": "ja",
                        "éŸ“æ–‡": "ko",
                        "å…¶ä»–": "custom"
                    }
                    
                    selected_lang = st.selectbox(
                        "é¸æ“‡èªè¨€",
                        options=list(languages.keys())
                    )
                    
                    if selected_lang == "å…¶ä»–":
                        custom_lang = st.text_input(
                            "è¼¸å…¥èªè¨€ä»£ç¢¼",
                            placeholder="ä¾‹å¦‚ï¼šfr ä»£è¡¨æ³•æ–‡",
                            help="è«‹è¼¸å…¥ ISO 639-1 èªè¨€ä»£ç¢¼"
                        )
                        language_code = custom_lang if custom_lang else None
                    else:
                        language_code = languages[selected_lang]
                else:
                    language_code = None
            
            # ElevenLabs ç›¸é—œè¨­å®š
            elevenlabs_api_key = None
            if transcription_service == "ElevenLabs":
                elevenlabs_api_key = st.text_input(
                    "ElevenLabs API é‡‘é‘°",
                    type="password"
                )
            
            # OpenAI API é‡‘é‘°
            openai_api_key = st.text_input(
                "OpenAI API é‡‘é‘°",
                type="password"
            )
            
            # OpenAI æ–°æ¨¡å‹ç›¸é—œè¨­å®š
            if transcription_service == "OpenAI 2025 New":
                openai_model = st.selectbox(
                    "é¸æ“‡ OpenAI è½‰éŒ„æ¨¡å‹",
                    ["gpt-4o-mini-transcribe", "gpt-4o-transcribe"],
                    index=0,
                    help="é¸æ“‡è¦ä½¿ç”¨çš„ OpenAI è½‰éŒ„æ¨¡å‹"
                )
                
                # èªè¨€è¨­å®š
                language_mode = st.radio(
                    "èªè¨€è¨­å®š",
                    options=["è‡ªå‹•åµæ¸¬", "æŒ‡å®šèªè¨€"],
                    help="é¸æ“‡éŸ³è¨Šçš„èªè¨€è™•ç†æ¨¡å¼"
                )
                
                if language_mode == "æŒ‡å®šèªè¨€":
                    languages = {
                        "ä¸­æ–‡ (ç¹é«”/ç°¡é«”)": "zh",
                        "è‹±æ–‡": "en",
                        "æ—¥æ–‡": "ja",
                        "éŸ“æ–‡": "ko",
                        "å…¶ä»–": "custom"
                    }
                    
                    selected_lang = st.selectbox(
                        "é¸æ“‡èªè¨€",
                        options=list(languages.keys())
                    )
                    
                    if selected_lang == "å…¶ä»–":
                        custom_lang = st.text_input(
                            "è¼¸å…¥èªè¨€ä»£ç¢¼",
                            placeholder="ä¾‹å¦‚ï¼šfr ä»£è¡¨æ³•æ–‡",
                            help="è«‹è¼¸å…¥ ISO 639-1 èªè¨€ä»£ç¢¼"
                        )
                        language_code = custom_lang if custom_lang else None
                    else:
                        language_code = languages[selected_lang]
                else:
                    language_code = None

            # å…¶ä»–è¨­å®š
            enable_diarization = st.checkbox("å•Ÿç”¨èªªè©±è€…è¾¨è­˜", value=False)
        
        # å„ªåŒ–è¨­å®šæ¨™ç±¤é 
        with tab2:
            # é¸æ“‡å„ªåŒ–æœå‹™
            optimization_service = st.selectbox(
                "é¸æ“‡å„ªåŒ–æœå‹™",
                ["Gemini", "OpenAI"],
                help="é¸æ“‡è¦ä½¿ç”¨çš„æ–‡å­—å„ªåŒ–æœå‹™"
            )
            
            # é¡¯ç¤ºæœå‹™èªªæ˜
            st.markdown(OPTIMIZATION_SERVICE_INFO[optimization_service])
            
            # é¡¯ç¤º Gemini æ¨¡å‹è³‡è¨Š
            if optimization_service == "Gemini":
                st.info("ä½¿ç”¨ Gemini 2.5 Pro Experimental æ¨¡å‹é€²è¡Œå„ªåŒ–")
            
            # Gemini API é‡‘é‘°ï¼ˆå¦‚æœé¸æ“‡ Geminiï¼‰
            gemini_api_key = None
            if optimization_service == "Gemini":
                gemini_api_key = st.text_input(
                    "Google API é‡‘é‘°",
                    type="password"
                )
            
            # å„ªåŒ–è¨­å®š
            temperature = st.slider(
                "å‰µæ„ç¨‹åº¦",
                0.0,
                1.0,
                0.5,
                help="è¼ƒé«˜çš„å€¼æœƒç”¢ç”Ÿæ›´æœ‰å‰µæ„çš„çµæœï¼Œè¼ƒä½çš„å€¼æœƒç”¢ç”Ÿæ›´ä¿å®ˆçš„çµæœ"
            )
        
        # ä½œè€…è³‡è¨Š
        st.markdown("---")
        st.markdown("""
        ### Created by
        **Tseng Yao Hsien**  
        Endocrinologist  
        Tungs' Taichung MetroHarbor Hospital
        """)

    # æç¤ºè©è¨­å®š
    with st.expander("æç¤ºè©è¨­å®šï¼ˆé¸å¡«ï¼‰", expanded=False):
        context_prompt = st.text_area(
            "è«‹è¼¸å…¥ç›¸é—œæç¤ºè©",
            placeholder="ä¾‹å¦‚ï¼š\n- é€™æ˜¯ä¸€æ®µé†«å­¸æ¼”è¬›\n- åŒ…å«å°ˆæœ‰åè©ï¼šç³–å°¿ç—…ã€èƒ°å³¶ç´ \n- ä¸»è¦è¨è«–ç³–å°¿ç—…çš„æ²»ç™‚æ–¹æ³•",
            help="æä¾›éŸ³è¨Šå…§å®¹çš„ç›¸é—œè³‡è¨Šï¼Œå¯ä»¥å¹«åŠ© AI æ›´æº–ç¢ºåœ°ç†è§£å’Œè½‰éŒ„å…§å®¹"
        )
    
    # ä¸Šå‚³æª”æ¡ˆ
    uploaded_file = st.file_uploader(
        "ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ",
        type=["mp3", "wav", "ogg", "m4a"]
    )
    
    # åªé¡¯ç¤ºè½‰éŒ„æŒ‰éˆ•
    transcribe_button = st.button("ğŸ™ï¸ è½‰éŒ„éŸ³è¨Š", use_container_width=True)
    
    # é¡¯ç¤ºè½‰éŒ„çµæœï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    if st.session_state.transcribed_text:
        st.subheader("è½‰éŒ„çµæœ")
        
        # é¡¯ç¤ºè½‰éŒ„æ–‡å­—
        st.text_area(
            "è½‰éŒ„æ–‡å­—",
            st.session_state.transcribed_text,
            height=200
        )
        
        # ä¸‹è¼‰æŒ‰éˆ•
        st.markdown("### ä¸‹è¼‰é¸é …")
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰è½‰éŒ„æ–‡å­—",
            data=st.session_state.transcribed_text,
            file_name="transcription.txt",
            mime="text/plain",
            help="ä¸‹è¼‰è½‰éŒ„å¾Œçš„æ–‡å­—æª”æ¡ˆ",
            use_container_width=True,
            key="download_transcription"
        )
        
        # åªåœ¨æœ‰è½‰éŒ„æ–‡å­—æ™‚é¡¯ç¤ºå„ªåŒ–æŒ‰éˆ•
        optimize_button = st.button("âœ¨ å„ªåŒ–æ–‡å­—", use_container_width=True)
    else:
        optimize_button = False
    
    # è™•ç†è½‰éŒ„
    if uploaded_file and transcribe_button:
        if not openai_api_key:
            st.error("è«‹æä¾› OpenAI API é‡‘é‘°")
            return
            
        if transcription_service == "ElevenLabs" and not elevenlabs_api_key:
            st.error("è«‹æä¾› ElevenLabs API é‡‘é‘°")
            return
        
        try:
            with st.spinner("è™•ç†ä¸­..."):
                # åˆå§‹åŒ–è®Šæ•¸
                full_transcript = ""
                
                # åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if transcription_service == "OpenAI 2025 New":
                    openai_client = OpenAI(api_key=openai_api_key)
                
                # è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ
                suffix = os.path.splitext(uploaded_file.name)[1]
                with tempfile.NamedTemporaryFile(
                    delete=False,
                    suffix=suffix
                ) as temp_file:
                    temp_file.write(uploaded_file.getvalue())
                    temp_path = temp_file.name
                
                try:
                    # æª¢æŸ¥éŸ³è¨Šé•·åº¦
                    audio = AudioSegment.from_file(temp_path)
                    duration_seconds = len(audio) / 1000
                    
                    if duration_seconds > 1500 and transcription_service == "OpenAI 2025 New":
                        # å¦‚æœéŸ³è¨Šè¶…é 1500 ç§’ä¸”ä½¿ç”¨ OpenAIï¼Œç›´æ¥é€²è¡Œæ™‚é•·åˆ†å‰²
                        audio_segments = split_large_audio(temp_path, max_duration_seconds=1400)
                        if not audio_segments:
                            st.error("æª”æ¡ˆåˆ†å‰²å¤±æ•—")
                            return
                    elif check_file_size(temp_path):
                        # å¦‚æœæª”æ¡ˆå¤ªå¤§ï¼ŒæŒ‰æª”æ¡ˆå¤§å°åˆ†å‰²
                        audio_segments = split_large_audio(temp_path)
                        if not audio_segments:
                            st.error("æª”æ¡ˆåˆ†å‰²å¤±æ•—")
                            return
                    else:
                        # æª”æ¡ˆå¤§å°å’Œé•·åº¦éƒ½åœ¨é™åˆ¶å…§ï¼Œç›´æ¥è™•ç†
                        audio_segments = [temp_path]
                    
                    progress_bar = st.progress(0)
                    for i, segment_path in enumerate(audio_segments):
                        if transcription_service == "Whisper":
                            result = transcribe_audio_whisper(
                                segment_path,
                                model_name=whisper_model,
                                language=language_code,
                                initial_prompt=context_prompt
                            )
                        elif transcription_service == "ElevenLabs":
                            result = transcribe_audio_elevenlabs(
                                api_key=elevenlabs_api_key,
                                file_path=segment_path,
                                language_code="zho",  # æŒ‡å®šä¸­æ–‡
                                diarize=enable_diarization
                            )
                        elif transcription_service == "OpenAI 2025 New":
                            with open(segment_path, "rb") as audio_file:
                                try:
                                    response = (
                                        openai_client.audio
                                        .transcriptions
                                        .create(
                                            model=openai_model,
                                            file=audio_file,
                                            language=language_code
                                        )
                                    )
                                    result = {"text": response.text}
                                except Exception as e:
                                    if "longer than 1500 seconds" in str(e):
                                        # å¦‚æœæª”æ¡ˆè¶…é 1500 ç§’ï¼Œé€²ä¸€æ­¥åˆ†å‰²
                                        sub_segments = split_large_audio(segment_path, max_duration_seconds=1400)
                                        if not sub_segments:
                                            st.error(f"åˆ†å‰²ç‰‡æ®µ {i+1} å¤±æ•—")
                                            continue
                                        
                                        sub_transcript = ""
                                        for sub_segment in sub_segments:
                                            with open(sub_segment, "rb") as sub_audio:
                                                sub_response = (
                                                    openai_client.audio
                                                    .transcriptions
                                                    .create(
                                                        model=openai_model,
                                                        file=sub_audio,
                                                        language=language_code
                                                    )
                                                )
                                                sub_transcript += sub_response.text + "\n"
                                            os.remove(sub_segment)
                                        result = {"text": sub_transcript}
                                    else:
                                        raise e
                        
                        if result:
                            full_transcript += result["text"] + "\n"
                        
                        # æ›´æ–°é€²åº¦
                        progress = (i + 1) / len(audio_segments)
                        progress_bar.progress(progress)
                        
                        os.remove(segment_path)
                finally:
                    # ç¢ºä¿æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                
                # è™•ç†è½‰éŒ„çµæœ
                if full_transcript:
                    st.session_state.transcribed_text = full_transcript
                    st.rerun()  # ä½¿ç”¨æ–°çš„ rerun æ–¹æ³•
                else:
                    st.error("è½‰éŒ„å¤±æ•—")
                    
        except Exception as e:
            st.error(f"è™•ç†å¤±æ•—ï¼š{str(e)}")
            logger.error(f"è™•ç†å¤±æ•—ï¼š{str(e)}")
    
    # è™•ç†å„ªåŒ–
    if st.session_state.transcribed_text and optimize_button:
        try:
            with st.spinner("å„ªåŒ–ä¸­..."):
                if optimization_service == "OpenAI":
                    if not openai_api_key:
                        st.error("è«‹æä¾› OpenAI API é‡‘é‘°")
                        return
                        
                    refined = refine_transcript(
                        raw_text=st.session_state.transcribed_text,
                        api_key=openai_api_key,
                        model="gpt-4o-mini",
                        temperature=temperature,
                        context=context_prompt
                    )
                else:  # Gemini
                    if not gemini_api_key:
                        st.error("è«‹æä¾› Google API é‡‘é‘°")
                        return
                        
                    refined = refine_transcript_gemini(
                        text=st.session_state.transcribed_text,
                        api_key=gemini_api_key,
                        temperature=temperature,
                        context=context_prompt
                    )
                
                if refined:
                    # å„²å­˜å„ªåŒ–çµæœåˆ° session state
                    st.session_state.optimized_text = refined["corrected"]
                    st.session_state.summary_text = refined["summary"]
                    
                    # ç§»é™¤ Markdown æ¨™è¨˜çš„å‡½æ•¸
                    def remove_markdown(text):
                        # ç§»é™¤æ¨™é¡Œç¬¦è™Ÿ (#)
                        text = text.replace('#', '')
                        # ç§»é™¤ç²—é«”æ¨™è¨˜ (**)
                        text = text.replace('**', '')
                        # ç§»é™¤æ–œé«”æ¨™è¨˜ (*)
                        text = text.replace('*', '')
                        # ç§»é™¤åˆ†éš”ç·š (---)
                        text = text.replace('---', '')
                        # ç§»é™¤å¤šé¤˜çš„ç©ºè¡Œ
                        text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())
                        return text
                    
                    # çµ„åˆå®Œæ•´çµæœæ–‡å­—ï¼ˆç´”æ–‡å­—æ ¼å¼ï¼Œç§»é™¤æ‰€æœ‰ Markdown æ¨™è¨˜ï¼‰
                    st.session_state.full_result = f"""å„ªåŒ–å¾Œæ–‡å­—ï¼š
{remove_markdown(refined["corrected"])}

é‡é»æ‘˜è¦ï¼š
{remove_markdown(refined["summary"])}"""

                    # Markdown æ ¼å¼çš„çµæœï¼ˆä¿ç•™ Markdown æ¨™è¨˜ï¼‰
                    st.session_state.markdown_result = f"""# å„ªåŒ–çµæœ

## å„ªåŒ–å¾Œæ–‡å­—
{refined["corrected"]}

## é‡é»æ‘˜è¦
{refined["summary"]}"""
                    
                    # æ›´æ–° token ä½¿ç”¨çµ±è¨ˆ
                    current_usage = refined.get("usage", {})
                    st.session_state.input_tokens = current_usage.get(
                        "total_input_tokens",
                        0
                    )
                    st.session_state.output_tokens = current_usage.get(
                        "total_output_tokens",
                        0
                    )
                    st.session_state.total_tokens = (
                        st.session_state.input_tokens +
                        st.session_state.output_tokens
                    )
                else:
                    st.error("æ–‡å­—å„ªåŒ–å¤±æ•—")
        except Exception as e:
            st.error(f"å„ªåŒ–å¤±æ•—ï¼š{str(e)}")
            logger.error(f"å„ªåŒ–å¤±æ•—ï¼š{str(e)}")

    # é¡¯ç¤ºå„ªåŒ–çµæœï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    if hasattr(st.session_state, 'optimized_text') and st.session_state.optimized_text:
        st.subheader("å„ªåŒ–çµæœ")
        
        # é¡¯ç¤ºå„ªåŒ–çµæœ
        st.text_area(
            "å®Œæ•´å„ªåŒ–çµæœ",
            st.session_state.full_result,
            height=500
        )
        
        # ä¸‹è¼‰æŒ‰éˆ•å€åŸŸ
        st.markdown("### ä¸‹è¼‰é¸é …")
        col1, col2 = st.columns(2)
        
        with col1:
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ç´”æ–‡å­—æ ¼å¼",
                data=st.session_state.full_result,  # å·²ç¶“æ˜¯ç´”æ–‡å­—æ ¼å¼ï¼Œä¸éœ€è¦é¡å¤–è™•ç†
                file_name="optimized_result.txt",
                mime="text/plain",
                help="ä¸‹è¼‰ç´”æ–‡å­—æ ¼å¼çš„å®Œæ•´çµæœï¼ˆåŒ…å«å„ªåŒ–çµæœå’Œæ‘˜è¦ï¼‰",
                use_container_width=True,
                key="download_optimized_txt"
            )
        
        with col2:
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ Markdown æ ¼å¼",
                data=st.session_state.markdown_result,
                file_name="optimized_result.md",
                mime="text/markdown",
                help="ä¸‹è¼‰ Markdown æ ¼å¼çš„å®Œæ•´çµæœï¼ˆåŒ…å«å„ªåŒ–çµæœå’Œæ‘˜è¦ï¼‰",
                use_container_width=True,
                key="download_optimized_md"
            )
        
        # é¡¯ç¤ºè²»ç”¨çµ±è¨ˆï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if optimization_service == "OpenAI":
            tokens_display = st.session_state.total_tokens
            st.markdown(f"ç¸½ Tokens: **{tokens_display:,}**")
            
            # è¨ˆç®—è²»ç”¨
            cost_result = calculate_cost(
                st.session_state.input_tokens,
                st.session_state.output_tokens,
                "gpt-4o-mini",
                is_cached=False
            )
            
            st.markdown(f"ç¸½è²»ç”¨: **NT$ {cost_result[1]:.2f}**")
            
            # é¡¯ç¤ºè©³ç´°æˆæœ¬è³‡è¨Š
            display_cost_info(
                st.session_state.input_tokens,
                st.session_state.output_tokens,
                "gpt-4o-mini",
                is_cached=False
            )
        else:
            st.info("Gemini API ä½¿ç”¨é‡æš«ä¸è¨ˆè²»")


if __name__ == "__main__":
    main() 